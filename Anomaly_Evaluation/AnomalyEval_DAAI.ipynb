{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "AnomalyEvaluation"
      ],
      "metadata": {
        "id": "CEluOb6F4Ic8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCtebHfW0-uo",
        "outputId": "3680a0e4-76f1-4430-994e-a8ef8064e0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/MyDrive/AI')"
      ],
      "metadata": {
        "id": "C_OfJonz2GjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from erfnet import ERFNet"
      ],
      "metadata": {
        "id": "gCLPZtsY2zPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ood_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqMbV5GG5f6v",
        "outputId": "3a8128d7-3e3f-4b54-d436-e8fe6821f6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ood_metrics\n",
            "  Downloading ood_metrics-1.1.2-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: matplotlib<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from ood_metrics) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.22 in /usr/local/lib/python3.10/dist-packages (from ood_metrics) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from ood_metrics) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0,>=3.0->ood_metrics) (1.16.0)\n",
            "Installing collected packages: ood_metrics\n",
            "Successfully installed ood_metrics-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import erfnet\n",
        "# !git clone https://github.com/shyam671/AnomalySegmentation_CourseProjectBaseCode.git"
      ],
      "metadata": {
        "id": "kxSbECMxCMSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcf5bd2-2628-4eda-89b0-6c7f53641cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AnomalySegmentation_CourseProjectBaseCode'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 61 (delta 8), reused 8 (delta 8), pack-reused 44\u001b[K\n",
            "Receiving objects: 100% (61/61), 21.48 MiB | 18.24 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# zip_file_path = '/content/drive/MyDrive/AnomalySegmentation_CourseProjectBaseCode.zip'\n",
        "# extracted_path = os.path.dirname(zip_file_path)\n",
        "\n",
        "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extracted_path)\n",
        "\n",
        "# print(f'File unzipped to: {extracted_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTCSCnz9ZzE3",
        "outputId": "1e2a07b7-cefe-446e-c1d9-be57817acafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File unzipped to: /content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1QhMGSkDJHk",
        "outputId": "2673f501-6d97-4af0-d1f5-f5c83ecf1b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=52ec536ee28c194ba09fa1a5213560666d9e5e77493d2d8e4267c409fe8c57fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNNsvid8gDaX",
        "outputId": "6da1a0e3-5584-4e1a-b68d-97bad54c74a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import wget\n",
        "sys.path.append('/content/drive/MyDrive/AnomalySegmentation_CourseProjectBaseCode/eval')\n",
        "sys.path.append('/content/drive/MyDrive/road-anomaly-benchmark-master/methods')\n",
        "\n",
        "import baselines\n",
        "from iouEval import *"
      ],
      "metadata": {
        "id": "MFEw7Do1igLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213b4d47-3eb9-4575-80ed-7b54fdd93a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import wget\n",
        "sys.path.append('/content/drive/MyDrive/AnomalySegmentation_CourseProjectBaseCode/eval')\n",
        "sys.path.append('/content/drive/MyDrive/road-anomaly-benchmark-master/methods')\n",
        "\n",
        "import baselines\n",
        "from iouEval import *\n",
        "import eval_iou\n",
        "from baselines import *\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import erfnet\n",
        "from erfnet import ERFNet\n",
        "import os.path as osp\n",
        "from argparse import ArgumentParser\n",
        "from ood_metrics import fpr_at_95_tpr, calc_metrics, plot_roc, plot_pr,plot_barcode\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "\n",
        "seed = 42\n",
        "\n",
        "# general reproducibility\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "# gpu training specific\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def main():\n",
        "    # parser = ArgumentParser()\n",
        "    # parser.add_argument(\n",
        "    #     \"--input\",\n",
        "    #     default=\"/content/drive/MyDrive/AI/Validation_Dataset/RoadObsticle21/images/*.webp\",\n",
        "    #     nargs=\"+\",\n",
        "    #     help=\"A list of space separated input images; \"\n",
        "    #     \"or a single glob pattern such as 'directory/*.jpg'\",\n",
        "    # )\n",
        "    # parser.add_argument('--loadDir',default=\"../trained_models/\")\n",
        "    # parser.add_argument('--loadWeights', default=\"erfnet_pretrained.pth\")\n",
        "    # parser.add_argument('--loadModel', default=\"erfnet.py\")\n",
        "    # parser.add_argument('--subset', default=\"val\")  #can be val or train (must have labels)\n",
        "    # parser.add_argument('--datadir', default=\"/content/drive/MyDrive/AI/leftImg8bit/val/frankfurt/*.png\")\n",
        "    # parser.add_argument('--num-workers', type=int, default=4)\n",
        "    # parser.add_argument('--batch-size', type=int, default=1)\n",
        "    # parser.add_argument('--cpu', action='store_true')\n",
        "    # args = parser.parse_args(args=[])\n",
        "\n",
        "    anomaly_score_list = []\n",
        "    ood_gts_list = []\n",
        "    means_iou = []\n",
        "    iou_per_class = []\n",
        "    input = \"/content/drive/MyDrive/Dataset/Validation_Dataset/Validation_Dataset/RoadAnomaly21/images/*.png\"\n",
        "    input_labels_masks = \"/content/drive/MyDrive/Dataset/Validation_Dataset/Validation_Dataset/RoadAnomaly21/labels_masks/*.png\"\n",
        "    image_files = glob.glob(input)\n",
        "    label_files = glob.glob(input_labels_masks)\n",
        "    loadDir = '/content/drive/MyDrive/AnomalySegmentation_CourseProjectBaseCode/trained_models'\n",
        "    loadWeights = '/content/drive/MyDrive/AnomalySegmentation_CourseProjectBaseCode/trained_models/erfnet_pretrained.pth'\n",
        "    loadModel = '/content/drive/MyDrive/AnomalySegmentation_CourseProjectBaseCode/eval/erfnet.py'\n",
        "    subset = 'val'\n",
        "    loaddir = ''\n",
        "    batch_size = 1\n",
        "    cpu = False\n",
        "\n",
        "    if not os.path.exists('results.txt'):\n",
        "        open('results.txt', 'w').close()\n",
        "    file = open('results.txt', 'a')\n",
        "\n",
        "    modelpath =  loadModel\n",
        "    weightspath =  loadWeights\n",
        "\n",
        "    print(\"Loading model: \" + modelpath)\n",
        "    print(\"Loading weights: \" + weightspath)\n",
        "\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    if (not cpu):\n",
        "      model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                if name.startswith(\"module.\"):\n",
        "                    own_state[name.split(\"module.\")[-1]].copy_(param)\n",
        "                else:\n",
        "                    # print(name, \" not loaded\")\n",
        "                    continue\n",
        "            else:\n",
        "                own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath, map_location=lambda storage, loc: storage))\n",
        "    print(\"Model and weights LOADED successfully\")\n",
        "    model.eval()\n",
        "\n",
        "    msp = Max_softmax(model)\n",
        "    # ent_max = Entropy_max(model)\n",
        "    # iou = iouEval(NUM_CLASSES)\n",
        "\n",
        "    # print = (len(label_files), len(image_files))\n",
        "\n",
        "    for idx, path in enumerate(image_files):\n",
        "        # print(path)\n",
        "        label_path = label_files[idx]\n",
        "        images = torch.from_numpy(np.array(Image.open(path).convert('RGB'))).unsqueeze(0).float()\n",
        "        images = images.permute(0,3,1,2)\n",
        "        label_mask = torch.from_numpy(np.array(Image.open(label_path))).unsqueeze(0).float()\n",
        "        label_mask = label_mask.unsqueeze(0)\n",
        "        label_mask = label_mask.permute(0,3,1,2)\n",
        "        with torch.no_grad():\n",
        "            result = model(images)\n",
        "\n",
        "        # anomaly_result = msp.anomaly_score(images)       #Max_Softmax_Probability(MSP) score analysis function\n",
        "        # anomaly_result = ent_max.anomaly_score(images)        #Max_Entropy score analysis function\n",
        "        anomaly_result = 1.0 - np.max(result.squeeze(0).data.cpu().numpy(), axis=0)     #Max_Logit score analysis function\n",
        "\n",
        "\n",
        "        # anomaly_score = Max_softmax.anomaly_score(image_np)\n",
        "        pathGT = path.replace(\"images\", \"labels_masks\")\n",
        "        if \"RoadObsticle21\" in pathGT:\n",
        "           pathGT = pathGT.replace(\"webp\", \"png\")\n",
        "        if \"fs_static\" in pathGT:\n",
        "           pathGT = pathGT.replace(\"jpg\", \"png\")\n",
        "        if \"RoadAnomaly\" in pathGT:\n",
        "           pathGT = pathGT.replace(\"jpg\", \"png\")\n",
        "\n",
        "        mask = Image.open(pathGT)\n",
        "        ood_gts = np.array(mask)\n",
        "\n",
        "        if \"RoadAnomaly\" in pathGT:\n",
        "            ood_gts = np.where((ood_gts==2), 1, ood_gts)\n",
        "        if \"LostAndFound\" in pathGT:\n",
        "            ood_gts = np.where((ood_gts==0), 255, ood_gts)\n",
        "            ood_gts = np.where((ood_gts==1), 0, ood_gts)\n",
        "            ood_gts = np.where((ood_gts>1)&(ood_gts<201), 1, ood_gts)\n",
        "\n",
        "        if \"Streethazard\" in pathGT:\n",
        "            ood_gts = np.where((ood_gts==14), 255, ood_gts)\n",
        "            ood_gts = np.where((ood_gts<20), 0, ood_gts)\n",
        "            ood_gts = np.where((ood_gts==255), 1, ood_gts)\n",
        "        ood_gts_ = torch.Tensor(ood_gts).unsqueeze(0).unsqueeze(0)\n",
        "        anomaly_result_ = torch.Tensor(anomaly_result).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        iou.reset()   #correct\n",
        "        # print(anomaly_result_.shape,ood_gts_.shape)\n",
        "        iou.addBatch(anomaly_result_, ood_gts_)  #correct\n",
        "        # miou, iou_arr = iou.getIoU\n",
        "        f_iou = iou.getIoU()  # correct\n",
        "\n",
        "        print(f'mean_IoU in each iteration: {f_iou[0].item()}')\n",
        "        print(f'IoU_per_class in each iteration: {f_iou[1][0].item()}')\n",
        "\n",
        "        # means_iou.append(f_iou[0].item())\n",
        "        # iou_per_class.append(f_iou[1][0].item())\n",
        "\n",
        "        if 1 not in np.unique(ood_gts):\n",
        "            continue\n",
        "        else:\n",
        "             ood_gts_list.append(ood_gts)\n",
        "             anomaly_score_list.append(anomaly_result)\n",
        "        del result, anomaly_result, ood_gts, mask\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # final_iou_mean = sum(means_iou) / len(means_iou)\n",
        "    file.write( \"\\n\")\n",
        "\n",
        "    ood_gts = np.array(ood_gts_list)\n",
        "    anomaly_scores = np.array(anomaly_score_list)\n",
        "\n",
        "    ood_mask = (ood_gts == 1)\n",
        "    ind_mask = (ood_gts == 0)\n",
        "\n",
        "    ood_out = anomaly_scores[ood_mask]\n",
        "    ind_out = anomaly_scores[ind_mask]\n",
        "\n",
        "    ood_label = np.ones(len(ood_out))\n",
        "    ind_label = np.zeros(len(ind_out))\n",
        "\n",
        "    val_out = np.concatenate((ind_out, ood_out))\n",
        "    val_label = np.concatenate((ind_label, ood_label))\n",
        "\n",
        "    prc_auc = average_precision_score(val_label, val_out)\n",
        "    fpr = fpr_at_95_tpr(val_out, val_label)\n",
        "\n",
        "    print(f'AUPRC score: {prc_auc*100.0}')\n",
        "    print(f'FPR@TPR95: {fpr*100.0}')\n",
        "\n",
        "    file.write(('    AUPRC score:' + str(prc_auc*100.0) + '   FPR@TPR95:' + str(fpr*100.0) ))\n",
        "    file.close()\n",
        "\n",
        "    # print(f'mean_IoU for all iteration is: {final_iou_mean}')\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "N8dVz4-8vUB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81025e4c-1d0c-47c9-989b-569e4e7df661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and weights LOADED successfully\n",
            "AUPRC score: 9.600823675910778\n",
            "FPR@TPR95: 93.78032503061641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python /content/AnomalySegmentation_CourseProjectBaseCode/eval/evalAnomaly.py --input '/content/drive/MyDrive/AI/Validation_Dataset/FS_LostFound_full/images/*.png' --loadDir ./"
      ],
      "metadata": {
        "id": "40PesFn1WVA1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}